# Word Munch

## Inspiration

I was reading dense documentation and hit a confusing sentence. I thought, "Did I get this right?"

Tab over to ChatGPT. Copy-paste my question and the confusing text. Get my answer. Tab back.

My flow? Dead. And worse - I'd turn around and completely forget what ChatGPT just told me. It wasn't MY thinking, so it never stuck.

A week later, another long paragraph. My brain goes: "Nah, too much work." More copy-paste cycles, more forgotten insights.

More time saved = more TikTok time. Win-win, right?

Wrong. I was training myself to be intellectually lazy. My reading muscles atrophied while my scroll finger got stronger.

Word Munch was born from a simple question: **What if I could get AI help without killing my reading flowâ€”and actually get better at reading instead of avoiding it?**

---

## What it does

Word Munch transforms any webpage into an intelligent reading coach that makes you BETTER at reading, not just faster at avoiding it.

### ðŸŽ¯ Core Value
Get AI help without breaking flow OR creating dependency

### Four Smart Features

- **Word Muncher:** Instant context-aware synonyms (no tab-switching)
- **Concept Muncher:** Write your understanding â†’ get color-coded feedback showing exactly what you got (green), partially missed (yellow), or completely missed (red)
- **Chunk Reading Modes:** 4 scientifically-designed focus environments
- **Cognitive Profile:** Tracks your reading patterns â†’ offers personalized reading insights

### The Magic
Unlike ChatGPT copy-paste that kills retention, Word Munch strengthens your comprehension skills while you read.

Unlike existing solutions that either create AI dependency (Summly, Elicit) or disrupt reading flow (Scholarly's automatic triggers), Word Munch is user-initiated and designed to strengthen comprehension skills rather than replace them.

---

## How I built it

### The Insight
Drawing from Cognitive Load Theory (Sweller, 1988), Active Processing Effect (Pressley, 2006), and Distributed Cognition principles (Luckin, 2018), I discovered that 70% of working memory gets wasted on basic processing during reading. Word Munch offloads the grunt work while preserving the mental effort needed for growth.

### Tech Stack
- **Frontend:** Chrome Extension + Manifest V3
- **Serverless Compute:** AWS Lambda + API Gateway (auto-scaling)
- **AI/ML:** Amazon Bedrock (Titan embeddings + Claude + Llama)
- **Storage & Caching:** DynamoDB + 3-tier caching (Memory â†’ IndexedDB â†’ DynamoDB)
- **Monitoring:** CloudWatch for performance tracking and cost optimization

### Why Serverless is Perfect Here
Reading assistance has a unique "burst-then-quiet" patternâ€”users might query 10 concepts in 5 minutes while deep in a document, then go silent for an hour. Traditional always-on servers waste resources during quiet periods, while serverless scales perfectly with this natural reading rhythm.

### Architecture Highlights

**Smart Activation:** No more copy-pasting entire paragraphs to ChatGPT. The system only activates when you need it, making serverless architecture the perfect fitâ€”AWS Lambda functions spring to life on demand, then hibernate until your next "help me" moment.

**Cost Optimization:** Smart AI invocation reduced costs by 60% while maintaining accuracyâ€”from $0.15 to $0.06 per reading session through intelligent model selection and caching.

**Lambda Optimization:** 3-tier caching strategy naturally solves cold start issues:
- **L1 - Memory Cache:** Frequent lookups cached in Lambda runtime memory (3-second TTL for immediate responses)
- **L2 - IndexedDB:** User-specific patterns cached locally in browser (24-hour TTL for personalized responses)  
- **L3 - DynamoDB:** Global knowledge base for long-tail queries across all users (persistent storage)

Different memory configurations for different AI tasks (512MB for synonyms, 1024MB+ for concept analysis) optimize cost per requestâ€”achieving <$0.001 per concept lookup with 99.9% cache hit rate for frequent terms.

---

## Challenges I ran into

### The Big Five

#### 1. Semantic Segmentation Goldilocks Problem
**Challenge:** Too small chunks â†’ false negatives, user frustration. Too large chunks â†’ can't pinpoint comprehension gaps  
**Solution:** Smart segmentation - embeddings for phrases/sentences, rule-based transition word detection for paragraphs

#### 2. Cost vs Accuracy Optimization
**Challenge:** Context selection + threshold tuning for expensive Claude calls  
**Solution:** Dynamic context strategy (full/minimal/extracted/none) + smart thresholds (very short + low similarity, most segments missed, or explicit user request)

#### 3. Instant Comprehension Gap Visualization
**Challenge:** How do users immediately see where their understanding broke down?  
**Solution:** Clean color-coded highlights (green/yellow/red) showing exactly what they got, partially grasped, or completely missed

#### 4. Real-time Performance in Manifest V3
**Challenge:** Sub-200ms response without lag in new Chrome extension environment  
**Solution:** 3-tier caching system (memory â†’ IndexedDB â†’ DynamoDB) to minimize latency

#### 5. Serverless Performance at Scale
**Challenge:** Balancing Lambda cold starts with cost efficiency during reading bursts  
**Solution:** Intelligent caching strategy - most concepts cached in-memory, predictive warming for related concepts, DynamoDB for long-tail queries

---

## Accomplishments that I'm proud of

### Technical Wins

âœ… **Intelligent segmentation:** Phrase/sentence-level embeddings accurately pinpoint comprehension gaps, while rule-based paragraph chunking groups related concepts naturallyâ€”precise enough for targeted feedback, efficient enough not to break the bank

âœ… **Smart AI invocation:** Optimized context selection + threshold tuning reduced Claude calls by 60% while maintaining feedback quality

âœ… **Visual comprehension mapping:** Clean color-coded highlights let users spot understanding gaps instantly  

âœ… **Sub-200ms performance:** 3-tier caching architecture (Memory â†’ IndexedDB â†’ DynamoDB) with intelligent cache promotion delivers real-time feedback without lag

âœ… **Intelligent cache management:** Automatic cleanup of expired entries, cache promotion from DB to memory, and per-user cache isolation

âœ… **Serverless cost efficiency:** Achieved 85% cost reduction vs traditional infrastructure ($0.06 vs $0.40 per session) by matching Lambda scaling to natural reading bursts

### Real Impact

âœ… **15-user pilot study:** 34% â†“ external AI dependency, 28% â†‘ comprehension scores

âœ… **Daily dogfooding:** I actually use this every day for technical docs

âœ… **Memory retention:** Users build understanding through their own thinking, not passive consumption

### The Sweet Spot
Built AI that makes users think BETTER, not AI that thinks FOR them.

---

## What I learned

### Key Insights

ðŸ§  **AI should augment thinking, not replace it** - The moment we outsource cognition entirely, we stop growing

ðŸ’° **Cost optimization is an art** - Every token matters when you're calling expensive models

ðŸŽ¯ **Best learning tools are invisible** - Users shouldn't feel like they're "learning," they should just get better

ðŸ“Š **Semantic similarity â‰  word matching** - Context and meaning trump surface-level text similarity

âš¡ **Serverless fits human patterns** - Reading behavior is naturally bursty, making serverless economics perfect for this use case

---

## What's next for Word Munch

### Immediate (Q3 2025)
- **PDF/e-book support** (expand beyond web pages)
- **Enhanced accuracy** with multimodal models
- **Advanced AWS integration:** S3 for document processing, EventBridge for reading analytics

### Scale Strategy
- **Target:** 50M+ knowledge workers (developers, researchers, analysts)
- **GTM:** Technical communities first â†’ broader professional market
- **Monetization:** Freemium model with advanced analytics
- **Technical foundation:** Full source code and deployment architecture documented for scalable implementation
- **Monitoring & Observability:** CloudWatch dashboards for real-time performance tracking and cost optimization

### The Vision
Make Word Munch invisible - so seamlessly integrated that better comprehension just... happens.

> *"Reading should feel like having a conversation with the smartest version of yourself."*